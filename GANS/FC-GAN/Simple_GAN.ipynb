{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T05:04:18.831622Z",
     "iopub.status.busy": "2025-09-27T05:04:18.831413Z",
     "iopub.status.idle": "2025-09-27T05:04:30.725464Z",
     "shell.execute_reply": "2025-09-27T05:04:30.724714Z",
     "shell.execute_reply.started": "2025-09-27T05:04:18.831606Z"
    },
    "id": "E9sbBnjwGbK3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Hyperparameters\n",
    "lr_gen = 1e-4        # Generator learning rate\n",
    "lr_disc = 4e-5       # Discriminator learning rate\n",
    "z_dim = 128\n",
    "img_dim = 28 * 28\n",
    "batch_size = 128\n",
    "epochs = 200\n",
    "\n",
    "# Output dir\n",
    "output_dir = \"/kaggle/working/generated_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ev54-1wTG4Z8"
   },
   "source": [
    "**Creating the discriminator and generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T05:04:30.726684Z",
     "iopub.status.busy": "2025-09-27T05:04:30.726323Z",
     "iopub.status.idle": "2025-09-27T05:04:30.732032Z",
     "shell.execute_reply": "2025-09-27T05:04:30.731486Z",
     "shell.execute_reply.started": "2025-09-27T05:04:30.726648Z"
    },
    "id": "D7CORMDYGy1m",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_dim):\n",
    "        super().__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(img_dim, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1),  # logits, no sigmoid\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Input:  (batch, z_dim)\n",
    "    Output: (batch, 28*28) in [-1, 1]\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim=128, img_dim=28*28):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256, bias=False),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(256, 512, bias=False),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(512, 1024, bias=False),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(1024, 512, bias=False),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(512, 256, bias=False),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(256, img_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KaaGfeH6IK-"
   },
   "source": [
    "**Initialising weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T05:04:30.768886Z",
     "iopub.status.busy": "2025-09-27T05:04:30.768616Z",
     "iopub.status.idle": "2025-09-27T05:04:30.786593Z",
     "shell.execute_reply": "2025-09-27T05:04:30.785914Z",
     "shell.execute_reply.started": "2025-09-27T05:04:30.768845Z"
    },
    "id": "z7277DVU6Miw",
    "outputId": "6c3f3637-8c00-4fcd-eef9-5e833075d2c3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n",
    "        if getattr(m, \"weight\", None) is not None:\n",
    "            nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        if getattr(m, \"bias\", None) is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "    elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "        if getattr(m, \"weight\", None) is not None:\n",
    "            nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        if getattr(m, \"bias\", None) is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "disc = Discriminator(img_dim).to(device)\n",
    "gen = Generator(z_dim=z_dim, img_dim=img_dim).to(device)\n",
    "\n",
    "# Apply weight initialization (define before calling apply)\n",
    "disc.apply(weights_init)\n",
    "gen.apply(weights_init)\n",
    "\n",
    "# Fixed noise for consistent visualization\n",
    "fixed_noise = torch.randn((batch_size, z_dim), device=device)\n",
    "\n",
    "print(\"Models initialized and moved to device\")\n",
    "print(f\"Discriminator parameters: {sum(p.numel() for p in disc.parameters()):,}\")\n",
    "print(f\"Generator parameters: {sum(p.numel() for p in gen.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eHe3qZgVp4S"
   },
   "source": [
    "**Prepping the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T05:04:30.788003Z",
     "iopub.status.busy": "2025-09-27T05:04:30.787350Z",
     "iopub.status.idle": "2025-09-27T05:04:30.885311Z",
     "shell.execute_reply": "2025-09-27T05:04:30.884685Z",
     "shell.execute_reply.started": "2025-09-27T05:04:30.787980Z"
    },
    "id": "Av2UirZvVsD2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset = datasets.MNIST(root=\"dataset/\", transform=transform, download=True)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Number of batches: {len(loader)}\")\n",
    "\n",
    "sample_batch = next(iter(loader))\n",
    "print(f\"Sample batch shape: {sample_batch[0].shape}\")\n",
    "print(f\"Sample data range: [{sample_batch[0].min():.3f}, {sample_batch[0].max():.3f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twawzrQznMrZ"
   },
   "source": [
    "**Defining losses and optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T05:04:30.886359Z",
     "iopub.status.busy": "2025-09-27T05:04:30.886158Z",
     "iopub.status.idle": "2025-09-27T05:04:31.200663Z",
     "shell.execute_reply": "2025-09-27T05:04:31.200057Z",
     "shell.execute_reply.started": "2025-09-27T05:04:30.886338Z"
    },
    "id": "H6b5VfbynL_d",
    "outputId": "3db16058-aa43-4f6c-e7a7-85370bc04422",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=lr_disc, betas=(0.5, 0.999))\n",
    "opt_gen  = optim.Adam(gen.parameters(),  lr=lr_gen,  betas=(0.5, 0.999))\n",
    "\n",
    "scheduler_disc = optim.lr_scheduler.ReduceLROnPlateau(opt_disc, mode='min', factor=0.5, patience=5)\n",
    "scheduler_gen  = optim.lr_scheduler.ReduceLROnPlateau(opt_gen,  mode='min', factor=0.5, patience=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0iRnO916va0"
   },
   "source": [
    "**Training loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-27T05:04:31.496Z"
    },
    "id": "GXKtDCgf3yyJ",
    "outputId": "3a45e05c-7e93-48ab-f7c9-6694a997e08f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Starting training on {device}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "best_lossG = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loop = tqdm(loader, leave=True, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    epoch_lossD = 0.0\n",
    "    epoch_lossG = 0.0\n",
    "    batches = 0\n",
    "\n",
    "    for batch_idx, (real, _) in enumerate(loop):\n",
    "        real = real.view(-1, 784).to(device)\n",
    "        bsz = real.size(0)\n",
    "\n",
    "        # Label smoothing within [0,1] for BCEWithLogitsLoss\n",
    "        real_labels = torch.empty(bsz, device=device).uniform_(0.7, 1.0)\n",
    "        fake_labels = torch.empty(bsz, device=device).uniform_(0.0, 0.3)\n",
    "\n",
    "        # Train Discriminator\n",
    "        noise = torch.randn(bsz, z_dim, device=device)\n",
    "        fake = gen(noise)\n",
    "\n",
    "        disc_real = disc(real).view(-1)\n",
    "        lossD_real = criterion(disc_real, real_labels)\n",
    "\n",
    "        disc_fake = disc(fake.detach()).view(-1)\n",
    "        lossD_fake = criterion(disc_fake, fake_labels)\n",
    "\n",
    "        lossD = 0.5 * (lossD_real + lossD_fake)\n",
    "        opt_disc.zero_grad()\n",
    "        lossD.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(disc.parameters(), max_norm=1.0)\n",
    "        opt_disc.step()\n",
    "\n",
    "        # Train Generator (wants disc(fake) to be 'real' = 1s)\n",
    "        output = disc(fake).view(-1)\n",
    "        lossG = criterion(output, torch.ones_like(output))\n",
    "        opt_gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(gen.parameters(), max_norm=1.0)\n",
    "        opt_gen.step()\n",
    "\n",
    "        epoch_lossD += lossD.item()\n",
    "        epoch_lossG += lossG.item()\n",
    "        batches += 1\n",
    "\n",
    "        loop.set_postfix(lossD=lossD.item(), lossG=lossG.item())\n",
    "\n",
    "    avg_lossD = epoch_lossD / batches\n",
    "    avg_lossG = epoch_lossG / batches\n",
    "\n",
    "    scheduler_disc.step(avg_lossD)\n",
    "    scheduler_gen.step(avg_lossG)\n",
    "\n",
    "    # Visualize and save images every 10 epochs (or first epoch)\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        with torch.no_grad():\n",
    "            fake_img = gen(fixed_noise).view(-1, 1, 28, 28).cpu()\n",
    "            fake_img = (fake_img + 1) / 2  # [0,1] for display\n",
    "            real_img = real[:64].view(-1, 1, 28, 28).cpu()\n",
    "            real_img = (real_img + 1) / 2  # [0,1]\n",
    "\n",
    "        grid_fake = torchvision.utils.make_grid(fake_img[:64], nrow=8, normalize=False)\n",
    "        grid_real = torchvision.utils.make_grid(real_img[:64], nrow=8, normalize=False)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1); plt.title(f\"Fake Images - Epoch {epoch+1}\"); plt.axis(\"off\")\n",
    "        plt.imshow(grid_fake.permute(1, 2, 0).squeeze(), cmap=\"gray\")\n",
    "        plt.subplot(1, 2, 2); plt.title(\"Real Images\"); plt.axis(\"off\")\n",
    "        plt.imshow(grid_real.permute(1, 2, 0).squeeze(), cmap=\"gray\")\n",
    "        plt.tight_layout()\n",
    "        save_path_img = os.path.join(output_dir, f\"epoch_{epoch+1:03d}_real_vs_fake.png\")\n",
    "        plt.savefig(save_path_img)\n",
    "        plt.show()\n",
    "\n",
    "    # Save best generator model checkpoint\n",
    "    if avg_lossG < best_lossG:\n",
    "        best_lossG = avg_lossG\n",
    "        model_save_path = os.path.join(output_dir, f\"best_gen_epoch_{epoch+1:03d}.pth\")\n",
    "        torch.save(gen.state_dict(), model_save_path)\n",
    "        print(f\"Saved best generator model at epoch {epoch+1} with loss {best_lossG:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
