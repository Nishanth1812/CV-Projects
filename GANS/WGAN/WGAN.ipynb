{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c69cdcc9",
   "metadata": {},
   "source": [
    "**Importing necessary modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f57cca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.datasets as datasets\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da10f23",
   "metadata": {},
   "source": [
    "**Setting up Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b63fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "learning_rate=5e-5\n",
    "batch_size=64\n",
    "image_size=64\n",
    "img_channels=3\n",
    "noise_dim=100\n",
    "num_epochs=5\n",
    "features_disc=64\n",
    "features_gen=64\n",
    "critic_iterations=5\n",
    "weight_clip=0.01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b6b147",
   "metadata": {},
   "source": [
    "**Building the discriminator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458dc40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,img_channels,features_d):\n",
    "        super(Discriminator,self).__init__() \n",
    "        \n",
    "        self.disc=nn.Sequential(\n",
    "            nn.Conv2d(img_channels,features_d,kernel_size=4,stride=2,padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            self._block(features_d,features_d*2,4,2,1),\n",
    "            self._block(features_d*2,features_d*4,4,2,1),\n",
    "            self._block(features_d*4,features_d*8,4,2,1),\n",
    "            \n",
    "            nn.Conv2d(features_d*8,1,kernel_size=4,stride=2,padding=0),\n",
    "        )\n",
    "        \n",
    "    def _block(self,in_channels,out_channels,kernel_size,stride,padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=kernel_size,stride=stride,padding=padding,bias=False,),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f89447",
   "metadata": {},
   "source": [
    "**Building the Generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63c28534",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,channels_noise,img_channels,features_g):\n",
    "        super(Generator,self).__init__()\n",
    "        \n",
    "        self.gen=nn.Sequential(\n",
    "            self._block(channels_noise,features_g*16,4,1,0),\n",
    "            self._block(features_g*16,features_g*8,4,2,1),\n",
    "            self._block(features_g*8,features_g*4,4,2,1),\n",
    "            self._block(features_g*4,features_g*2,4,2,1),\n",
    "            \n",
    "            nn.ConvTranspose2d(features_g*2,img_channels,kernel_size=4,stride=2,padding=1),\n",
    "            \n",
    "            nn.Tanh(),\n",
    "        )\n",
    "    \n",
    "    def _block(self,in_channels,out_channels,kernel_size,stride,padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=in_channels,out_channels=out_channels,kernel_size=kernel_size,stride=stride,padding=padding,bias=False),\n",
    "            nn.ReLU(),\n",
    "        ) \n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.gen(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514caffc",
   "metadata": {},
   "source": [
    "**Initialising weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8820c1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m,(nn.Conv2d,nn.ConvTranspose2d,nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data,0.0,0.02)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d553e57",
   "metadata": {},
   "source": [
    "**Loading dataset and setting up transforms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9de7951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success, tests passed!\n"
     ]
    }
   ],
   "source": [
    "transforms=transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5 for _ in range(img_channels)],\n",
    "            [0.5 for _ in range(img_channels)]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\"\"\"Loading the CelebA dataset with optimizations\"\"\"\n",
    "\n",
    "# Load the full CelebA dataset\n",
    "celeba_dataset = datasets.ImageFolder(\n",
    "    root=\"/kaggle/input/celeba-dataset/img_align_celeba\", \n",
    "    transform=transforms\n",
    ")\n",
    "\n",
    "# Optimized DataLoader with more workers and prefetching\n",
    "dataLoader=DataLoader(\n",
    "    dataset=celeba_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    "    prefetch_factor=2,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "# Verify dataset channels\n",
    "print(f\"Dataset image channels: {img_channels}\")\n",
    "print(f\"Dataset size: {len(celeba_dataset)}\")\n",
    "print(f\"Sample batch shape check:\")\n",
    "sample_batch, _ = next(iter(dataLoader))\n",
    "print(f\"Sample batch shape: {sample_batch.shape}\")\n",
    "\n",
    "# Force CPU to work by doing a quick pass through the data\n",
    "print(\"Warming up data loading...\")\n",
    "for i, (batch, _) in enumerate(dataLoader):\n",
    "    if i >= 5:\n",
    "        break\n",
    "print(\"Data loading optimized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f826970",
   "metadata": {},
   "source": [
    "**Initialising the networks,optimizers and datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7e064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initialising the networks\"\"\"\n",
    "\n",
    "gen=Generator(noise_dim,img_channels=img_channels,features_g=features_gen).to(device=device)\n",
    "\n",
    "disc=Discriminator(img_channels=img_channels,features_d=features_disc).to(device=device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    gen = nn.DataParallel(gen)\n",
    "    disc = nn.DataParallel(disc)\n",
    "\n",
    "# Test the models with sample input\n",
    "test_noise=torch.randn(1,noise_dim,1,1).to(device=device)\n",
    "test_real=torch.randn(1,img_channels,image_size,image_size).to(device=device)\n",
    "\n",
    "print(f\"Generator output shape: {gen(test_noise).shape}\")\n",
    "print(f\"Discriminator real output shape: {disc(test_real).shape}\")\n",
    "print(f\"Discriminator fake output shape: {disc(gen(test_noise)).shape}\")\n",
    "\n",
    "\"\"\"Initialising the optimizers\"\"\" \n",
    "\n",
    "opt_disc=optim.RMSprop(disc.parameters(),lr=learning_rate)\n",
    "\n",
    "opt_gen=optim.RMSprop(gen.parameters(),lr=learning_rate)\n",
    "\n",
    "\"\"\"Initialising the fixed noise\"\"\"\n",
    "fixed_noise=torch.randn(32,noise_dim,1,1).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0200820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "step=0\n",
    "disc.train()\n",
    "gen.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8556c2",
   "metadata": {},
   "source": [
    "**Training loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135f9acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "best_gen_loss = float('inf')\n",
    "best_disc_loss = float('inf')\n",
    "\n",
    "total_steps = num_epochs * len(dataLoader)\n",
    "pbar = tqdm(total=total_steps, desc='Training', dynamic_ncols=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss_critic = 0.0\n",
    "    epoch_loss_gen = 0.0\n",
    "    \n",
    "    for batch_idx,(real,_) in enumerate(dataLoader):\n",
    "        real=real.to(device)\n",
    "        bsz = real.size(0)\n",
    "        \n",
    "        for _ in range(critic_iterations):\n",
    "            noise=torch.randn(bsz,noise_dim,1,1).to(device)\n",
    "            fake=gen(noise)\n",
    "            \n",
    "            critic_real=disc(real).reshape(-1)\n",
    "            critic_fake=disc(fake.detach()).reshape(-1)\n",
    "            loss_critic= -(torch.mean(critic_real)- torch.mean(critic_fake))\n",
    "            disc.zero_grad()\n",
    "            loss_critic.backward(retain_graph=True)\n",
    "            opt_disc.step()\n",
    "            \n",
    "            for p in disc.parameters():\n",
    "                p.data.clamp_(-weight_clip,weight_clip)\n",
    "                \n",
    "        epoch_loss_critic += loss_critic.item()\n",
    "                \n",
    "        output=disc(fake).reshape(-1)\n",
    "        loss_gen=-(torch.mean(output))\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "        \n",
    "        epoch_loss_gen += loss_gen.item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'epoch': epoch+1,\n",
    "            'Loss D': f'{loss_critic.item():.4f}',\n",
    "            'Loss G': f'{loss_gen.item():.4f}'\n",
    "        })\n",
    "        pbar.update(1)\n",
    "    \n",
    "    avg_loss_critic = epoch_loss_critic / len(dataLoader)\n",
    "    avg_loss_gen = epoch_loss_gen / len(dataLoader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        fake_img = gen(fixed_noise)\n",
    "        real_grid = vutils.make_grid(real[:16], nrow=4, normalize=True)\n",
    "        fake_grid = vutils.make_grid(fake_img[:16], nrow=4, normalize=True)\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        axes[0].imshow(real_grid.permute(1, 2, 0).cpu())\n",
    "        axes[0].axis('off')\n",
    "        axes[0].set_title('Real')\n",
    "        axes[1].imshow(fake_grid.permute(1, 2, 0).cpu())\n",
    "        axes[1].axis('off')\n",
    "        axes[1].set_title('Fake')\n",
    "        fig.savefig(f'outputs/epoch_{epoch+1}_real_vs_fake.png', bbox_inches='tight', pad_inches=0)\n",
    "        plt.close(fig)\n",
    "    \n",
    "    # Safety: ensure models were not accidentally overwritten by a tensor\n",
    "    if not isinstance(gen, nn.Module) or not isinstance(disc, nn.Module):\n",
    "        raise RuntimeError(f\"Generator or Discriminator overwritten. Types: gen={type(gen)}, disc={type(disc)}\")\n",
    "    \n",
    "    # Explicit cast so static analysis recognizes modules; prevents Tensor shadowing issues\n",
    "    from typing import cast\n",
    "    gen_to_save = cast(nn.Module, gen.module if hasattr(gen, 'module') else gen)\n",
    "    disc_to_save = cast(nn.Module, disc.module if hasattr(disc, 'module') else disc)\n",
    "    assert isinstance(gen_to_save, nn.Module) and isinstance(disc_to_save, nn.Module), \\\n",
    "        f\"Unexpected types: gen_to_save={type(gen_to_save)}, disc_to_save={type(disc_to_save)}\"\n",
    "    \n",
    "    if avg_loss_gen < best_gen_loss:\n",
    "        best_gen_loss = avg_loss_gen\n",
    "        torch.save(gen_to_save.state_dict(), 'outputs/best_generator.pth')\n",
    "    \n",
    "    if avg_loss_critic < best_disc_loss:\n",
    "        best_disc_loss = avg_loss_critic\n",
    "        torch.save(disc_to_save.state_dict(), 'outputs/best_discriminator.pth')\n",
    "\n",
    "pbar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GANS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
