{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fda9502",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f57cca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a94c82",
   "metadata": {},
   "source": [
    "**Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b63fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 64\n",
    "IMAGE_SIZE = 64\n",
    "CHANNELS_IMG = 3\n",
    "Z_DIM = 100\n",
    "NUM_EPOCHS = 100\n",
    "FEATURES_CRITIC = 16\n",
    "FEATURES_GEN = 16\n",
    "CRITIC_ITERATIONS = 5\n",
    "LAMBDA_GP = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4d72a5",
   "metadata": {},
   "source": [
    "**Gradient Penalty**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccb6b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, real, fake, device=\"cpu\"):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = real * alpha + fake.detach() * (1 - alpha)\n",
    "    interpolated_images.requires_grad_(True)\n",
    "    mixed_scores = critic(interpolated_images)\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3be894f",
   "metadata": {},
   "source": [
    "**Discriminator (Critic)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458dc40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Conv2d(channels_img, features_d, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            self._block(features_d, features_d * 2, 4, 2, 1),\n",
    "            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n",
    "            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n",
    "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.InstanceNorm2d(out_channels, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feb78fb",
   "metadata": {},
   "source": [
    "**Generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c28534",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            self._block(channels_noise, features_g * 16, 4, 1, 0),\n",
    "            self._block(features_g * 16, features_g * 8, 4, 2, 1),\n",
    "            self._block(features_g * 8, features_g * 4, 4, 2, 1),\n",
    "            self._block(features_g * 4, features_g * 2, 4, 2, 1),\n",
    "            nn.ConvTranspose2d(features_g * 2, channels_img, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c740b66e",
   "metadata": {},
   "source": [
    "**Weight Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8820c1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eda95b",
   "metadata": {},
   "source": [
    "**Dataset Transforms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9de7951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success, tests passed!\n"
     ]
    }
   ],
   "source": [
    "transforms_pipeline = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.CenterCrop(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5] * CHANNELS_IMG, [0.5] * CHANNELS_IMG),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22d2ff2",
   "metadata": {},
   "source": [
    "**Setup Networks and Checkpoints**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7e064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    datasets.ImageFolder(root=\"/kaggle/input/celeba-dataset/img_align_celeba\", transform=transforms_pipeline),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "print(\"Verifying dataset dimensions...\")\n",
    "sample_batch = next(iter(loader))\n",
    "print(f\"Real batch shape: {sample_batch[0].shape}\")\n",
    "del sample_batch\n",
    "\n",
    "gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
    "critic = Discriminator(CHANNELS_IMG, FEATURES_CRITIC).to(device)\n",
    "initialize_weights(gen)\n",
    "initialize_weights(critic)\n",
    "\n",
    "print(\"Verifying generator output...\")\n",
    "test_noise = torch.randn(1, Z_DIM, 1, 1).to(device)\n",
    "test_fake = gen(test_noise)\n",
    "print(f\"Fake batch shape: {test_fake.shape}\")\n",
    "del test_noise, test_fake\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    gen = nn.DataParallel(gen)\n",
    "    critic = nn.DataParallel(critic)\n",
    "\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
    "opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
    "\n",
    "fixed_noise = torch.randn(32, Z_DIM, 1, 1).to(device)\n",
    "step = 0\n",
    "\n",
    "gen.train()\n",
    "critic.train()\n",
    "\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "best_gen_loss = float('inf')\n",
    "best_critic_loss = float('inf')\n",
    "start_epoch = 0\n",
    "checkpoint_path = \"checkpoints/latest_checkpoint.pth\"\n",
    "best_gen_path = \"outputs/best_generator.pth\"\n",
    "best_critic_path = \"outputs/best_critic.pth\"\n",
    "\n",
    "def get_model_state_dict(model):\n",
    "    return model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n",
    "\n",
    "def load_model_state_dict(model, state_dict):\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        model.module.load_state_dict(state_dict)\n",
    "    else:\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    load_model_state_dict(gen, checkpoint['gen_state'])\n",
    "    load_model_state_dict(critic, checkpoint['critic_state'])\n",
    "    opt_gen.load_state_dict(checkpoint['opt_gen_state'])\n",
    "    opt_critic.load_state_dict(checkpoint['opt_critic_state'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    best_gen_loss = checkpoint['best_gen_loss']\n",
    "    best_critic_loss = checkpoint['best_critic_loss']\n",
    "    step = checkpoint['step']\n",
    "    print(f\"Resumed from epoch {start_epoch}\")\n",
    "else:\n",
    "    print(\"Starting fresh training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7a2a4f",
   "metadata": {},
   "source": [
    "**Training Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0200820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as time_module\n",
    "\n",
    "epoch_times = []\n",
    "training_start_time = time_module.time()\n",
    "\n",
    "print(f\"Device: {device} | GPUs: {torch.cuda.device_count()}\")\n",
    "print(f\"Config: BS={BATCH_SIZE}, IMG={IMAGE_SIZE}x{IMAGE_SIZE}, Epochs={NUM_EPOCHS}\")\n",
    "print(f\"Est. time: ~220h (~9.2 days) | Opt.: ~100-150h (~4.2-6.3 days)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2749b59",
   "metadata": {},
   "source": [
    "**Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135f9acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, NUM_EPOCHS):\n",
    "    epoch_start_time = time_module.time()\n",
    "    epoch_gen_losses = []\n",
    "    epoch_critic_losses = []\n",
    "    \n",
    "    for batch_idx, (real, _) in enumerate(tqdm(loader)):\n",
    "        real = real.to(device)\n",
    "        cur_batch_size = real.shape[0]\n",
    "\n",
    "        for _ in range(CRITIC_ITERATIONS):\n",
    "            noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(device)\n",
    "            fake = gen(noise)\n",
    "            critic_real = critic(real).reshape(-1)\n",
    "            critic_fake = critic(fake.detach()).reshape(-1)\n",
    "            gp = gradient_penalty(critic, real, fake, device=device)\n",
    "            loss_critic = -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP * gp\n",
    "            critic.zero_grad()\n",
    "            loss_critic.backward()\n",
    "            opt_critic.step()\n",
    "\n",
    "        epoch_critic_losses.append(loss_critic.item())\n",
    "\n",
    "        noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(device)\n",
    "        fake = gen(noise)\n",
    "        gen_fake = critic(fake).reshape(-1)\n",
    "        loss_gen = -torch.mean(gen_fake)\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        epoch_gen_losses.append(loss_gen.item())\n",
    "\n",
    "        if batch_idx % 100 == 0 and batch_idx > 0:\n",
    "            print(f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(loader)} | D: {loss_critic:.4f}, G: {loss_gen:.4f}\")\n",
    "            step += 1\n",
    "\n",
    "    avg_gen_loss = sum(epoch_gen_losses) / len(epoch_gen_losses)\n",
    "    avg_critic_loss = sum(epoch_critic_losses) / len(epoch_critic_losses)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fake = gen(fixed_noise)\n",
    "        img_grid = torchvision.utils.make_grid(fake[:32], normalize=True)\n",
    "        vutils.save_image(img_grid, f\"outputs/epoch_{epoch:03d}.png\")\n",
    "    \n",
    "    gen_state = get_model_state_dict(gen)\n",
    "    critic_state = get_model_state_dict(critic)\n",
    "    \n",
    "    if avg_gen_loss < best_gen_loss:\n",
    "        best_gen_loss = avg_gen_loss\n",
    "        torch.save(gen_state, best_gen_path)\n",
    "        print(f\"✓ Best generator (loss: {best_gen_loss:.4f})\")\n",
    "    \n",
    "    if avg_critic_loss < best_critic_loss:\n",
    "        best_critic_loss = avg_critic_loss\n",
    "        torch.save(critic_state, best_critic_path)\n",
    "        print(f\"✓ Best critic (loss: {best_critic_loss:.4f})\")\n",
    "    \n",
    "    checkpoint = {\n",
    "        'epoch': epoch + 1,\n",
    "        'gen_state': gen_state,\n",
    "        'critic_state': critic_state,\n",
    "        'opt_gen_state': opt_gen.state_dict(),\n",
    "        'opt_critic_state': opt_critic.state_dict(),\n",
    "        'best_gen_loss': best_gen_loss,\n",
    "        'best_critic_loss': best_critic_loss,\n",
    "        'step': step,\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    \n",
    "    epoch_time = time_module.time() - epoch_start_time\n",
    "    epoch_times.append(epoch_time)\n",
    "    avg_epoch_time = sum(epoch_times) / len(epoch_times)\n",
    "    remaining_epochs = NUM_EPOCHS - (epoch + 1)\n",
    "    eta_seconds = remaining_epochs * avg_epoch_time\n",
    "    eta_hours = eta_seconds / 3600\n",
    "    eta_days = eta_hours / 24\n",
    "    elapsed_time = time_module.time() - training_start_time\n",
    "    elapsed_hours = elapsed_time / 3600\n",
    "    \n",
    "    print(f\"Epoch [{epoch}/{NUM_EPOCHS}] | G: {avg_gen_loss:.4f} | D: {avg_critic_loss:.4f} | Time: {epoch_time/60:.1f}m | ETA: {eta_hours:.1f}h ({eta_days:.2f}d)\\n\")\n",
    "\n",
    "print(f\"\\n✓ Training complete! Total: {(time_module.time() - training_start_time)/3600:.1f}h\")\n",
    "print(f\"Best models: {best_gen_path} & {best_critic_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f324658",
   "metadata": {},
   "source": [
    "**Visualize Training Progress**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c15d3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "sample_files = sorted(glob.glob(\"outputs/epoch_*.png\"))\n",
    "if sample_files:\n",
    "    num_to_show = min(4, len(sample_files))\n",
    "    fig, axes = plt.subplots(1, num_to_show, figsize=(15, 4))\n",
    "    if num_to_show == 1:\n",
    "        axes = [axes]\n",
    "    for idx, sample_file in enumerate(sample_files[-num_to_show:]):\n",
    "        img = Image.open(sample_file)\n",
    "        axes[idx].imshow(img)\n",
    "        epoch_num = sample_file.split('_')[-1].replace('.png', '')\n",
    "        axes[idx].set_title(f\"Epoch {epoch_num}\")\n",
    "        axes[idx].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GANS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
